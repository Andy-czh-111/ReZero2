import os
import argparse
import torch
import numpy as np
import soundfile as sf
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

# å¼•å…¥é¡¹ç›®æ¨¡å—
from model.system import ReZeroSystem
from utils.istft import ISTFT
# compute_metrics ä¼šåœ¨éœ€è¦æ—¶å»¶è¿Ÿå¯¼å…¥ï¼ˆé¿å…å¤–éƒ¨ä¾èµ–åœ¨æ— ç¯å¢ƒä¸‹æŠ¥é”™ï¼‰

# === é…ç½® ===
DEFAULT_CHECKPOINT = "/Project/nerual_beamforming/myproject/ReZero2/checkpoints/ReZero_Final_Run_12261705/best_model.pth"
DEFAULT_OUTPUT = "./results"
DEFAULT_SPEECH_DIR = "/Project/Separation/Data/LibriSpeech/train-clean-360"
DEFAULT_NOISE_DIR = "/Project/Separation/Data/Musan"

def load_model(device, checkpoint_path=None):
    R = 0.025
    MIC_LOCS = np.array([
    [R * np.cos(2 * np.pi * i / 8), R * np.sin(2 * np.pi * i / 8), 0.0] 
    for i in range(8)
])
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = ReZeroSystem(
        mic_locations=torch.from_numpy(MIC_LOCS).float(),
        bsrnn_channels=48,
        task_type='angle'
    )
    
    # åŠ è½½æƒé‡
    ckpt_to_load = checkpoint_path if checkpoint_path is not None else DEFAULT_CHECKPOINT
    if os.path.exists(ckpt_to_load):
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {ckpt_to_load}")
        checkpoint = torch.load(ckpt_to_load, map_location=device)
        # å…¼å®¹ DataParallel çš„æƒé‡é”®å (å»é™¤ 'module.' å‰ç¼€)
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        new_state_dict = {}
        for k, v in state_dict.items():
            name = k.replace("module.", "") 
            new_state_dict[name] = v
        model.load_state_dict(new_state_dict)
    else:
        print(f"âš ï¸ æ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {ckpt_to_load}. ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹è¿›è¡Œæ¨ç† (ä»…ç”¨äº smoke-test)ã€‚")
    
    model.to(device)
    model.eval()
    return model

def inference(checkpoint_path=None, speech_dir=None, noise_dir=None, output_dir=None, use_dataset=False, device_str=None):
    device = torch.device(device_str) if device_str is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    out_dir = output_dir if output_dir is not None else DEFAULT_OUTPUT
    if not os.path.exists(out_dir): os.makedirs(out_dir, exist_ok=True)

    # 1. åŠ è½½æ¨¡å‹
    model = load_model(device, checkpoint_path=checkpoint_path)
    istft = ISTFT(win_len=512, win_shift_ratio=0.25, nfft=512).to(device)

    # 2. å‡†å¤‡æµ‹è¯•æ•°æ® (ä½¿ç”¨ Dataset ç”Ÿæˆä¸€ä¸ªæ ·æœ¬)
    # æ³¨æ„: ä¸ºäº†æµ‹è¯•æ³›åŒ–æ€§ï¼Œæœ€å¥½ä½¿ç”¨è®­ç»ƒé›†ä¸­æ²¡è§è¿‡çš„æ–‡ä»¶
    import glob
    sdir = speech_dir if speech_dir is not None else DEFAULT_SPEECH_DIR
    ndir = noise_dir if noise_dir is not None else DEFAULT_NOISE_DIR
    speech_list = glob.glob(os.path.join(sdir, "**/*.flac"), recursive=True) + glob.glob(os.path.join(sdir, "**/*.wav"), recursive=True)
    noise_list = glob.glob(os.path.join(ndir, "**/*.wav"), recursive=True)

    # å°è¯•ä½¿ç”¨ Dataset è·å–æ ·æœ¬ï¼›å¦‚æœæ²¡æœ‰æ•°æ®æˆ–è¯»å–å¤±è´¥ï¼Œå›é€€ä¸ºåˆæˆæ ·æœ¬
    sample = None
    try:
        if len(speech_list) > 0 and len(noise_list) > 0:
            # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…åœ¨æ²¡æœ‰ pyroomacoustics çš„ç¯å¢ƒä¸­ç›´æ¥æŠ¥é”™
            try:
                from data.dataset import ReZeroOnTheFlyDataset
            except Exception as e:
                print(f"æ— æ³•å¯¼å…¥ ReZeroOnTheFlyDataset: {e}")
                raise

            test_dataset = ReZeroOnTheFlyDataset(speech_list[:10], noise_list[:10], fs=16000)
            print("æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ ·æœ¬ (æ¥è‡ª Dataset)...")
            sample = test_dataset[0]
    except Exception as e:
        print(f"ä» Dataset è·å–æ ·æœ¬å¤±è´¥: {e}")

    if sample is None:
        print("ä½¿ç”¨åˆæˆæ ·æœ¬ä½œä¸ºå›é€€ (éšæœºä¿¡å·)ã€‚")
        T = 16000
        Ch = 8
        mix = torch.randn(1, T, Ch).to(device)
        target = torch.randn(1, T).mul(0.05).to(device)
        Q = 1
        region_params = {
            'azi_low': torch.tensor([-0.5]).to(device),
            'azi_high': torch.tensor([0.5]).to(device),
            'ele_low': torch.tensor([0.0]).to(device),
            'ele_high': torch.tensor([1.57]).to(device),
            'dist_low': torch.tensor([0.0]).to(device),
            'dist_high': torch.tensor([2.0]).to(device)
        }
    else:
        # å¢åŠ  Batch ç»´åº¦
        mix = sample['mix'].unsqueeze(0).to(device)       # (1, T, M)
        target = sample['target'].unsqueeze(0).to(device) # (1, T)
        Q = sample['Q'].item()
        region_params = sample['region']
        for k in region_params:
            region_params[k] = region_params[k].unsqueeze(0).to(device)

    # 3. æ¨ç† (Inference)
    with torch.no_grad():
        # è¿™é‡Œéœ€è¦ç”¨ä½ çš„ AMP åŒ…è£…æˆ–ç›´æ¥è·‘ (æ¨ç†é€šå¸¸ä¸éœ€è¦ AMP)
        est_stft = model(mix, region_params)
        est_wav = istft(est_stft, length=mix.shape[1]) # (1, T)

    # 4. ä¿å­˜éŸ³é¢‘
    mix_np = mix[0, :, 0].cpu().numpy() # åªä¿å­˜ç¬¬0ä¸ªé€šé“çš„æ··åˆéŸ³é¢‘
    est_np = est_wav[0].cpu().numpy()
    target_np = target[0].cpu().numpy()
    
    # æ–‡ä»¶åä¿¡æ¯
    azi_l = np.rad2deg(region_params['azi_low'].item())
    azi_h = np.rad2deg(region_params['azi_high'].item())
    prefix = f"Q{Q}_Azi{int(azi_l)}_{int(azi_h)}"
    
    sf.write(os.path.join(out_dir, f"{prefix}_mix.wav"), mix_np, 16000)
    sf.write(os.path.join(out_dir, f"{prefix}_est.wav"), est_np, 16000)
    sf.write(os.path.join(out_dir, f"{prefix}_ref.wav"), target_np, 16000)
    
    print(f"\nâœ… éŸ³é¢‘å·²ä¿å­˜åˆ° {DEFAULT_OUTPUT}/")
    print(f"  - Mix: {prefix}_mix.wav (æ··åˆéŸ³é¢‘)")
    print(f"  - Est: {prefix}_est.wav (æ¨¡å‹è¾“å‡º)")
    print(f"  - Ref: {prefix}_ref.wav (å‚è€ƒç›®æ ‡)")

    # 5. è®¡ç®—æŒ‡æ ‡ï¼ˆå°è¯•å¯¼å…¥ evaluate.compute_metricsï¼Œå¦åˆ™ä½¿ç”¨å›é€€å®ç°ï¼‰
    print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    try:
        from evaluate import compute_metrics
    except Exception:
        def compute_metrics(est_wav, target_wav, mix_wav, Q, fs=16000):
            # æœ¬åœ°å›é€€å®ç°ï¼šdecay / ç®€å• SNR
            eps = 1e-9
            metrics = {}
            if Q == 0:
                energy_in = np.sum(mix_wav**2) + eps
                energy_out = np.sum(est_wav**2) + eps
                metrics['decay'] = 10 * np.log10(energy_out / energy_in)
            else:
                # ç®€å• SNR è¿‘ä¼¼
                num = np.sum(target_wav**2) + eps
                den = np.sum((target_wav - est_wav)**2) + eps
                metrics['sdr'] = 10 * np.log10(num / den)
                metrics['stoi'] = 0.0
                metrics['pesq'] = 1.0
            return metrics

    try:
        metrics = compute_metrics(est_np, target_np, mix_np, Q)
        for k, v in metrics.items():
            try:
                print(f"  - {k}: {v:.4f}")
            except Exception:
                print(f"  - {k}: {v}")

        if Q > 0 and metrics.get('sdr', -99) < 5.0:
            print("\nâš ï¸ è­¦å‘Š: SDR å¾ˆä½ (< 5dB)ã€‚æ¨¡å‹å¯èƒ½æœªæˆåŠŸåˆ†ç¦»ã€‚")

    except Exception as e:
        print(f"æ— æ³•è®¡ç®—æŒ‡æ ‡: {e}")

    # 6. (å¯é€‰) ç»˜åˆ¶æ³¢å½¢å›¾
    plt.figure(figsize=(10, 6))
    plt.subplot(3, 1, 1); plt.plot(mix_np); plt.title("Mixture (Ch0)"); plt.grid()
    plt.subplot(3, 1, 2); plt.plot(target_np); plt.title(f"Target (Q={Q})"); plt.grid()
    plt.subplot(3, 1, 3); plt.plot(est_np); plt.title("Estimated"); plt.grid()
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"{prefix}_waveform.png"))
    print(f"  - Plot: {prefix}_waveform.png")

def _parse_args():
    p = argparse.ArgumentParser(description='ReZero inference (smoke-test friendly)')
    p.add_argument('--checkpoint', type=str, default=None, help='Path to model checkpoint')
    p.add_argument('--speech_dir', type=str, default=None, help='Path to speech dataset (optional)')
    p.add_argument('--noise_dir', type=str, default=None, help='Path to noise dataset (optional)')
    p.add_argument('--output_dir', type=str, default=None, help='Output dir for wavs and plots')
    p.add_argument('--use_dataset', action='store_true', help='Try to use ReZeroOnTheFlyDataset if available')
    p.add_argument('--device', type=str, default=None, help='Device string (e.g., cpu or cuda:0)')
    return p.parse_args()

if __name__ == "__main__":
    args = _parse_args()
    inference(checkpoint_path=args.checkpoint, speech_dir=args.speech_dir, noise_dir=args.noise_dir,
              output_dir=args.output_dir, use_dataset=args.use_dataset, device_str=args.device)